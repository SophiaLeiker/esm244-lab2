---
title: "Lab2"
author: "Sophia Leiker"
date: "1/13/2022"
output: html_document
---

# Objectives

- practice comparing the performance of different linear regression models using AIC and cross-validation
- learn to use formulas in R models
- use a for-loop to perform cross-validation manually

```{r setup, include=TRUE, message=FALSE, warning=FALSE}
knitr::opts_chunk$set(echo = TRUE, message = FALSE, warning = FALSE) #this is for the following code chunks

library(tidyverse)
library(palmerpenguins)
library(AICcmodavg)
library(equatiomatic)
```

# Create a model to predict penguin mass based on visually observable characteristics 

Create a model we can use in the field to estimate penguins mass based on readily observable characteristics, based on data in the `palmerpenguins` package

```{r}
penguins_clean <- penguins %>% 
  drop_na() %>% 
  rename(mass = body_mass_g,
         bill_l = bill_length_mm,
         bill_d = bill_depth_mm,
         flip_l = flipper_length_mm)

mdl1 <- lm(mass ~ bill_l + bill_d + flip_l + species + sex + island,
           data = penguins_clean)
```

Let's use a formula (this does the same things as the lines 35 and 36 above)
```{r}
f1 <- mass ~ bill_l + bill_d + flip_l + species + sex + island
mdl1 <- lm(f1, data = penguins_clean)

f2 <- mass ~ bill_l + bill_d + flip_l + species + sex
mdl2 <- lm(f2, data = penguins_clean)

f3 <- mass ~ bill_l + flip_l + species + sex
mdl3 <- lm(f3, data = penguins_clean)
```

## Let's use AIC to compare and determine best model
- Looking at the Adjusted R^2 value they are all around 87%
- Use the AIC function (need to be difference by at least 2 to consider one better than another)
- In the results summary table, we are looking at model 2 as being the best, next best model is model1 which is ~3.5 lower than that, model 3 is not significantly better than model 1 (difference is less than 2)

```{r}
AIC(mdl1, mdl2, mdl3)

#This is the AIC correction
AICcmodavg::AICc(mdl1) #4727.925

#this gives us a model summary and ranks them in order of preference
AICcmodavg::aictab(list(mdl1, mdl2, mdl3))
```

# K-fold cross validation
- Cross validation allows us to hold out a set of data (train:test split)

```{r}
folds <- 10
fold_vec <- rep(1:folds, length.out = nrow(penguins_clean))
table(fold_vec)

set.seed(42)

#this is pulling apart the dataset so we can do a train test split, in this case the test dataframe is 34 objects, the train dataframe is 299 objects
penguins_fold <- penguins_clean %>% 
  mutate(group = sample(fold_vec, size = n(), replace = FALSE))

### First fold
test_df <- penguins_fold %>% 
  filter(group == 1)
train_df <- penguins_fold %>% 
  filter(group !=1)
```


